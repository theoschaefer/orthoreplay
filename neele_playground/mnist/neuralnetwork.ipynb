{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network on MNIST Dataset\n",
    "\n",
    "This script implements training and testing a neural network on the MNIST dataset for the final report of \\\n",
    "**Neele Elbersgerd (1496121)** in the course \\\n",
    "Real and Artificial Neural Networks, University of Melbourne. \\\n",
    "The code is in partly taken and adapted from *Make Your Own Neural Network* (c) Tariq Rashid, 2016, license is GPLv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---import packages\n",
    "import numpy as np\n",
    "import scipy\n",
    "import time\n",
    "import pickle # for storing results easily\n",
    "import glob # for searching folders\n",
    "import os # for soft paths\n",
    "from os.path import join as opj\n",
    "from groo.groo import get_root\n",
    "\n",
    "#---set path\n",
    "# I use the groo package for setting up a root folder \n",
    "# and have relatives paths from there\n",
    "# if you want to run this as a guest, put root = \"your/path\"\n",
    "root = get_root(\".neuralroot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ANN class\n",
    "Define a class that initialises neural network objects; adapted from Rashid, 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralnet:\n",
    "    def __init__(self, inodes: int, hnodes: int, onodes: int, lr: float) -> None:\n",
    "        self.inodes = inodes\n",
    "        self.hnodes = hnodes\n",
    "        self.onodes = onodes\n",
    "        self.lr = lr\n",
    "\n",
    "        # initialise weights with normally distributed randoms\n",
    "        self.wih = np.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes,self.inodes))\n",
    "        self.who = np.random.normal(0.0, pow(self.hnodes, -0.5), (self.onodes,self.hnodes))\n",
    "\n",
    "        # sigmoid as activation function\n",
    "        self.activationf = lambda x: scipy.special.expit(x)\n",
    "    \n",
    "    def __str__(self): #descriptor method\n",
    "        desc = f\"This neural network has {self.inodes} input, {self.hnodes} hidden, and {self.onodes} output nodes. \"\n",
    "        desc = desc + f\"The learning rate is: {self.lr}. \"\n",
    "        desc = desc + f\"The activation function used is sigmoid with random normally distributed weights.\"\n",
    "        return desc\n",
    "    \n",
    "    def train(self, inputs: list, targets: list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputarr = np.array(inputs, ndmin=2).T\n",
    "        targetarr = np.array(targets, ndmin=2).T\n",
    "\n",
    "        # calculate signals\n",
    "        hidden_in = np.dot(self.wih, inputarr) #into hidden layer \n",
    "        hidden_out = self.activationf(hidden_in) #from hidden layer\n",
    "        final_in = np.dot(self.who, hidden_out) #into final output layer\n",
    "        final_out = self.activationf(final_in) #from final output layer\n",
    "\n",
    "        # output layer error is (target - actual)\n",
    "        output_err = targetarr - final_out\n",
    "        # hidden layer error is output_err, split by weights, recombined at hidden nodes\n",
    "        hidden_err = np.dot(self.who.T, output_err) \n",
    "\n",
    "        # update the weights for the links between the hidden and output layers\n",
    "        self.who += self.lr * np.dot((output_err * final_out * (1.0 - final_out)), np.transpose(hidden_out))\n",
    "        # update the weights for the links between the input and hidden layers\n",
    "        self.wih += self.lr * np.dot((hidden_err * hidden_out * (1.0 - hidden_out)), np.transpose(inputarr))\n",
    "\n",
    "    def query(self, inputs: list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = np.array(inputs, ndmin=2).T\n",
    "\n",
    "        # calculate signals \n",
    "        hidden_in = np.dot(self.wih, inputs) #into hidden layer\n",
    "        hidden_out = self.activationf(hidden_in) #from hidden layer\n",
    "        final_in = np.dot(self.who, hidden_out) #into final output layer\n",
    "        final_out = self.activationf(final_in) #from final output layer\n",
    "        return final_out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist(ann, epochs: int):\n",
    "    \"\"\"Implement training of a neuralnet object based on the original MNIST dataset.\n",
    "\n",
    "    Args:\n",
    "        ann (neuralnet): object from neuralnet class\n",
    "        epochs (int): number of epochs to run training on\n",
    "    \"\"\"    \n",
    "    # load the augmented mnist training data CSV file into a list\n",
    "    train_file = open(opj(root, \"project\", \"data\", \"mnist_train.csv\"), 'r')\n",
    "    train_data = train_file.readlines()\n",
    "    train_file.close()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        # go through all records in the training data set\n",
    "        for record in train_data:\n",
    "            all_values = record.split(',')  #split the record\n",
    "            inputs = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01 #scale inputs\n",
    "            # create the target output values (all 0.01, except the desired label which is 0.99)\n",
    "            targets = np.zeros(ann.onodes) + 0.01\n",
    "            targets[int(float(all_values[0]))] = 0.99\n",
    "            ann.train(inputs, targets)\n",
    "        print(f\"-- Epoch {e+1} of {epochs} done --\")\n",
    "    print(\"-- Done with training --\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mnist(ann, filename: str):\n",
    "    \"\"\"Implement testing of a neuralnet object based on the augmented MNIST dataset.\n",
    "\n",
    "    Args:\n",
    "        ann (neuralnet): object from neuralnet class\n",
    "        filename (str): filename of the dataset to use in testing\n",
    "\n",
    "    Returns:\n",
    "        ytrue (list): true labels of test set\n",
    "        ypred (list): predicted labels of test set\n",
    "        confmat (numpy array): confusion matrix\n",
    "        confidence (numpy array): matrix of average confidence values\n",
    "    \"\"\"    \n",
    "    # load the original mnist test data into a list\n",
    "    test_file = open(opj(root, \"project\", \"data\", filename+\".csv\"), 'r')\n",
    "    test_data = test_file.readlines()\n",
    "    test_file.close()\n",
    "    \n",
    "    # initialise performance measures:\n",
    "    ytrue = [] # true labels\n",
    "    ypred = [] # prediction of ann\n",
    "    confmat = np.zeros((ann.onodes,ann.onodes))  # confusion matrix\n",
    "    confidence = np.zeros((ann.onodes,ann.onodes)) # confidence matrix\n",
    "    \n",
    "    # go through all the records in the test data set\n",
    "    for record in test_data:\n",
    "        all_values = record.split(',') # split the record by commas\n",
    "        correct_label = int(all_values[0]) # correct label\n",
    "        ytrue.append(correct_label) # add correct label\n",
    "        inputs = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01 # scale the inputs\n",
    "        outputs = ann.query(inputs) #query the network\n",
    "        guess = np.argmax(outputs) #index of highest value is the guess\n",
    "        ypred.append(guess) #add network's guess\n",
    "        confmat[correct_label][guess] += 1 #add count to cell of confusion matrix\n",
    "        # add output vector (confidence) to the confidence matrix\n",
    "        confidence[correct_label:(correct_label+1)] = confidence[correct_label:(correct_label+1)] + outputs.T\n",
    "    \n",
    "    #row-wise sum (how many images of each digit in test data)\n",
    "    count = np.sum(confmat, axis=1) \n",
    "    #average confidence matrix to show the ann's average confidence for each cell\n",
    "    confidence = confidence / count\n",
    "    return ytrue, ypred, confmat, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define function for performance score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_mnist(ytrue: list, ypred: list):\n",
    "    \"\"\"assessing neural network performance by generating a performance score.\n",
    "\n",
    "    Args:\n",
    "        ytrue (list): true labels of test set\n",
    "        ypred (list): predicted labels of test set\n",
    "\n",
    "    Returns:\n",
    "        performance (float)\n",
    "    \"\"\"    \n",
    "    score = [1 if i==j else 0 for i,j in zip(ytrue, ypred)]\n",
    "    score = np.asarray(score)\n",
    "    performance = score.sum() / score.size\n",
    "    print(f\"-- The performance is {performance} --\")\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network\n",
    "In the following cell, a neural network will be trained on the original MNIST data set. \\\n",
    "The hyperparameters, as well as training time, will be saved to the dictionary *res*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This neural network has 784 input, 200 hidden, and 10 output nodes. The learning rate is: 0.1. The activation function used is sigmoid with random normally distributed weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1 of 1 done --\n",
      "-- Done with training --\n"
     ]
    }
   ],
   "source": [
    "#---initialise result dictionary to store model parameters and results\n",
    "res = {}\n",
    "res[\"hidden_nodes\"] = 200\n",
    "res[\"learning_rate\"] = 0.1\n",
    "res[\"epochs\"] = 1\n",
    "\n",
    "#---instance of neural network\n",
    "ann = neuralnet(inodes=784, hnodes=res[\"hidden_nodes\"], onodes=10, lr=res[\"learning_rate\"])\n",
    "print(ann)\n",
    "\n",
    "#---train ann\n",
    "start = time.time() # start time\n",
    "train_mnist(ann, epochs = res[\"epochs\"])\n",
    "time_train = time.time() - start # end time\n",
    "res[\"train_time\"] = time_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the network\n",
    "The next part implements the testing of the network with different (augmented) test datasets. \\\n",
    "I use the package pickle to save python objects (the dictionary *res*) to a file to be able to read them in later as the same data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---get all augmented testfile names in the folder\n",
    "files = opj(root, \"project\", \"data\", f\"augdata*.csv\")\n",
    "files = list(map(os.path.basename, glob.glob(files)))\n",
    "files = [i[:-4] for i in files if \"META\" not in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Testing the ANN on mnist_test --\n",
      "-- The performance is 0.9659090909090909 --\n"
     ]
    }
   ],
   "source": [
    "#---loop to get results for all augmented test sets\n",
    "for f in files:\n",
    "    res[\"testfile\"] = f\n",
    "    print(f\"-- Testing the ANN on {f} --\")\n",
    "\n",
    "    #---test\n",
    "    start = time.time() # start time\n",
    "    ytrue, ypred, confmat, confidence = test_mnist(ann, filename = res[\"testfile\"])\n",
    "    time_test = time.time()-start # end time\n",
    "    res[\"test_time\"] = time_test\n",
    "\n",
    "    #---save performance indices\n",
    "    res[\"performance\"] = perform_mnist(ytrue, ypred)\n",
    "    res[\"ypred\"] = ypred\n",
    "    res[\"ytrue\"] = ytrue\n",
    "    res[\"confmat\"] = confmat\n",
    "    res[\"confidence\"] = confidence\n",
    "\n",
    "    #---save results\n",
    "    with open(opj(root, \"project\", \"data\", \"results\", (f+\"RES.pkl\")), \"wb\") as fb:\n",
    "        pickle.dump(res, fb, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
